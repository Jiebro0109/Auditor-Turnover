{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build office-level auditor turnover panel data**\n",
    "* Jie Xia, SUSTech\n",
    "* 2025-02-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean illegal characters in csv\n",
    "* When importing the Revelio CSV into STATA, it was noticed that some rows were misinterpreted due to illegal characters in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "# 1. Read the CSV file\n",
    "file_path = r\"E:\\USA auditor turnover data\\merged_ouput_position.csv\\merged_ouput_position.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Sort by user_id in ascending order\n",
    "df_sorted = df.sort_values(by=\"user_id\", ascending=True)\n",
    "\n",
    "# 3. Define a function to remove illegal characters\n",
    "def remove_illegal_chars(value):\n",
    "    if isinstance(value, str):\n",
    "        return \"\".join(char for char in value if char.isprintable())  # Keep only printable characters\n",
    "    return value  # Leave non-string values unchanged\n",
    "\n",
    "# 4. Apply the function to the entire DataFrame\n",
    "df_sorted = df_sorted.applymap(remove_illegal_chars)\n",
    "\n",
    "# 5. Ensure the output directory exists\n",
    "output_dir = r\"E:\\USA auditor turnover data\\turnover data\"\n",
    "\n",
    "df_sorted.to_csv(os.path.join(output_dir, \"merged_ouput_position_cleaned.csv\"), index=False)\n",
    "\n",
    "print(\"Finished cleaning and saving csv files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Individual-year panel data\n",
    "* Remove outliers and duplicates\n",
    "* Expand the data across years\n",
    "* Sort the DataFrame by user_id and year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load in Revelio csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source_data = r\"E:\\USA auditor turnover data\\turnover data\\merged_ouput_position_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(source_data, engine=\"pyarrow\") # use \"pyarrow\" can evidently accelerate the process\n",
    "print(\"Successfully read csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Show basic information of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print basic info of csv\n",
    "# df.head(10)\n",
    "# df.describe()\n",
    "# df.columns\n",
    "# df.dtypes\n",
    "print(f\"The shape(rows, columns) of dataframe is {df.shape}\")\n",
    "print(f\"{df.duplicated().sum()} rows are duplicated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cleaning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clean missing and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete Missing and duplicated rows\n",
    "# 1. Delete missing startdate rows\n",
    "row1 = len(df)\n",
    "df_cleaned = df.dropna(subset=['startdate'])\n",
    "row2 = len(df_cleaned)\n",
    "\n",
    "print(f\"{row1 - row2} rows dropped because of missing startdate\")\n",
    "print(f\"And {row2} rows left\")\n",
    "\n",
    "# 2. Delete duplicated rows\n",
    "row3 = len(df_cleaned)\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "row4 = len(df_cleaned)\n",
    "\n",
    "print(f\"{row3 - row4} rows dropped because of duplicated\")\n",
    "print(f\"And {row4} rows left\")\n",
    "\n",
    "# 3. Drop location and auditor missing rows\n",
    "len1 = len(df_cleaned)\n",
    "df_cleaned = df_cleaned.dropna(subset=['location_raw'])\n",
    "len2 = len(df_cleaned)\n",
    "\n",
    "len3 = len(df_cleaned)\n",
    "df_cleaned = df_cleaned.dropna(subset=['AUDITOR_FKEY'])\n",
    "len4 = len(df_cleaned)\n",
    "\n",
    "len5 = len(df_cleaned)\n",
    "df_cleaned = df_cleaned.dropna(subset=['AUDITOR_NAME'])\n",
    "len6 = len(df_cleaned)\n",
    "\n",
    "print(f\"{len1 - len2} rows dropped because of missing location_raw\")\n",
    "print(f\"And {len2} rows left\")\n",
    "print(f\"{len3 - len4} rows dropped because of missing AUDITOR_FKEY\")\n",
    "print(f\"And {len4} rows left\")\n",
    "print(f\"{len5 - len6} rows dropped because of missing AUDITOR_NAME\")\n",
    "print(f\"And {len6} rows left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill missing value in `enddate` column with 2024-12-31 to create 'present' date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill missing cell in enddate with 2024-12-31 (present)\n",
    "\n",
    "print(f\"{df_cleaned[\"enddate\"].isna().sum()} rows of enddate columns are NaN.\")\n",
    "\n",
    "df_cleaned[\"enddate\"] = df_cleaned[\"enddate\"].fillna(\"2024-12-31\")\n",
    "\n",
    "print(f\"{df_cleaned[\"enddate\"].isna().sum()} rows of enddate columns are NaN after filling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extract year from date variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract year variables\n",
    "# 1. Ensure the date columns are datetime type\n",
    "df_cleaned[\"startdate\"] = pd.to_datetime(df_cleaned[\"startdate\"])\n",
    "df_cleaned[\"enddate\"] = pd.to_datetime(df_cleaned[\"enddate\"])\n",
    "\n",
    "# 2. Check startdate and enddate\n",
    "latest_start = df_cleaned[\"startdate\"].max()  # Most recent (latest) start date\n",
    "earliest_start = df_cleaned[\"startdate\"].min()  # Oldest (earliest) start date\n",
    "\n",
    "latest_end = df_cleaned[\"enddate\"].max()  # Most recent (latest) end date\n",
    "earliest_end = df_cleaned[\"enddate\"].min()  # Oldest (earliest) end date\n",
    "\n",
    "# Print results with clear labels\n",
    "print(f\"Latest (most recent) start date: {latest_start}\")\n",
    "print(f\"Earliest (oldest) start date: {earliest_start}\")\n",
    "\n",
    "print(f\"Latest (most recent) end date: {latest_end}\")\n",
    "print(f\"Earliest (oldest) end date: {earliest_end}\")\n",
    "\n",
    "# 3. Extract year from date\n",
    "df_cleaned[\"year_start\"] = df_cleaned[\"startdate\"].dt.year\n",
    "df_cleaned[\"year_end\"] = df_cleaned[\"enddate\"].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Expand the years to create panel data\n",
    "* Expand the year range from `year_start` to `year_end` into a consecutive series of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from pandasgui import show\n",
    "### Build individual-year panel data\n",
    "\"\"\"\n",
    "# 1. Expand the rows by year (too slow)\n",
    "expanded_rows = []\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    for year in range(row[\"year_start\"], row[\"year_end\"] + 1):  # generate year ranges\n",
    "        new_row = row.copy()  # copy entire rows\n",
    "        new_row[\"year\"] = year  # generate discrete year\n",
    "        expanded_rows.append(new_row)\n",
    "\"\"\"\n",
    "# 1. Expand the rows by year\n",
    "expanded_rows = []\n",
    "columns = list(df_cleaned.columns) + [\"year\"]  # Preserve original columns and add \"year\"\n",
    "print(\"Start to expand the rows\")\n",
    "\n",
    "# Iterate over rows using itertuples() (much faster than iterrows())\n",
    "for row in tqdm(df_cleaned.itertuples(index=False), total=len(df_cleaned), desc=\"Expanding Rows\"):\n",
    "    for year in range(row.year_start, row.year_end + 1):  # Generate year range\n",
    "        new_row = list(row) + [year]  # Copy entire row and add year column\n",
    "        expanded_rows.append(new_row)\n",
    "print(\"Successfully construct panel data\")\n",
    "\n",
    "# 2. Convert the expanded list into a DataFrame\n",
    "df_panel = pd.DataFrame(expanded_rows, columns=columns)\n",
    "print(f\"The shape of dataframe is {df_panel.shape}\")\n",
    "\n",
    "# 3. Delete useless columns\n",
    "df_panel = df_panel.drop(columns=[\"startdate\", \"enddate\", \"year_start\", \"year_end\"])\n",
    "\n",
    "# 4. Sort by id and year\n",
    "df_panel = df_panel.sort_values(by=[\"user_id\", \"year\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame(about 32 mins)\n",
    "individual_panel = r\"E:\\USA auditor turnover data\\turnover data\\individual_panel.csv\"\n",
    "\n",
    "print(f\"Start to write output csv file\")\n",
    "df_panel.to_csv(individual_panel, index=False)\n",
    "\n",
    "print(f\"Successfully save individual_panel at {individual_panel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate office-level variables\n",
    "* AUDITOR_FLOW: Number of auditors who departed from(or join in) an audit office scaled by the number of auditors in an office-year\n",
    "  \n",
    "  * A flow-out auditor is defined as:\n",
    "    * 1. Auditors who left the office\n",
    "    * 2. Auditors who still work in same office but no longer work as an auditor\n",
    "  \n",
    "  * A flow-in auditor is defined as:\n",
    "    * 1. New auditors who first join in the office\n",
    "    * 2. Old employee who first start to work as an auditor in the office \n",
    "  \n",
    "* HIGH_SALARY: if the salary offered by an audit office is greater than the sample median in an MSA\n",
    "  \n",
    "* MSA_OFFICES: Number of audit offices in an MSA in a year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load the individual-year panel csv file\n",
    "* Use chunking to load the file in parts\n",
    "* Only include rows where the region is 'USA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasgui import show\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now()\n",
    "input_csv = r\"E:\\USA auditor turnover data\\turnover data\\individual_panel.csv\"\n",
    "\n",
    "chunk_size = 100000  # Read 100,000 rows per chunk\n",
    "filtered_chunks = []  # List to store filtered chunks\n",
    "\n",
    "print(f\"Begin to load in file at {current_time}\")\n",
    "# Read the CSV file in chunks to optimize memory usage (about 2 mins)\n",
    "for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
    "    chunk_filtered = chunk[chunk[\"country\"] == \"United States\"]  # Filter rows where \"country\" is \"United States\"\n",
    "    filtered_chunks.append(chunk_filtered)  # Store the filtered chunk\n",
    "\n",
    "# Concatenate all filtered chunks into a single DataFrame\n",
    "df_indivi_us = pd.concat(filtered_chunks, ignore_index=True)\n",
    "\n",
    "row_indivi = len(df_indivi_us)  # Get the total number of rows after filtering\n",
    "print(\"Successfully loaded file\")\n",
    "print(f\"Shape of df: {df_indivi_us.shape}\")  # Display the shape of the final DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filter individual panel using list from AA daa\n",
    "* Filter individual panel using AA dta file based on AUDITOR_FKEY (have limited to USA office in STATA)\n",
    "* Retain only the AUDITOR_FKEY values in the individual panel that have corresponding records in the AA dta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load in Audit Analytics data\n",
    "current_time = datetime.now()\n",
    "input_dta = r\"E:\\USA auditor turnover data\\Audit Analytics data\\AuditAnalytics_AUDITOR_USA.csv\"\n",
    "\n",
    "print(f\"Begin to load in file at {current_time}\")\n",
    "df_aa = pd.read_csv(input_dta)\n",
    "\n",
    "print(\"Successfully load in file\")\n",
    "print(f\"Shape of df is {df_aa.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filter the AUDITOR by AA file\n",
    "current_time = datetime.now()\n",
    "df_filtered = df_indivi_us[df_indivi_us['AUDITOR_FKEY'].isin(df_aa['AUDITOR_FKEY'])]\n",
    "\n",
    "# Calculate number of unmatched rows of df_indivi_us\n",
    "unmatched_indivi = df_indivi_us[~df_indivi_us['AUDITOR_FKEY'].isin(df_aa['AUDITOR_FKEY'])].shape[0]\n",
    "\n",
    "# Calculate number of unmatched rows of df_aa\n",
    "unmatched_aa = df_aa[~df_aa['AUDITOR_FKEY'].isin(df_indivi_us['AUDITOR_FKEY'])].shape[0]\n",
    "\n",
    "row_filtered = len(df_filtered)\n",
    "\n",
    "print(f\"Successfully matched at {current_time}\\n\"\n",
    "      f\"New df shape is {df_filtered.shape}\\n\"\n",
    "      f\"{row_indivi - row_filtered} rows dropped from df_indivi_us\\n\"\n",
    "      f\"{unmatched_indivi} rows in df_indivi_us were not matched\\n\"\n",
    "      f\"{unmatched_aa} rows in df_aa were not matched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataframe, release memory\n",
    "del df_indivi_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Clean Revelio dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Standardize office location expression (city-level)\n",
    "* When manually checking the `location_raw` column in revelio labs data, it was observed that the same city may by expressed in different ways. Therefore, standardizing these expressions is necessary to later build a consistent `office_key_location` and `office_id` latter.\n",
    "  \n",
    "1. Extract City Name:\n",
    "\n",
    "   * Extract the substring before a comma from location_raw and convert it to title case.\n",
    "\n",
    "2. Clean the Text:\n",
    "\n",
    "   * Trim whitespace, remove parentheses (and their content), strip punctuation, eliminate extra spaces, remove unwanted words, replace abbreviations, and correct typos—all while ensuring proper capitalization.\n",
    "\n",
    "3. Standardize City Names:\n",
    "\n",
    "   * Replace any variants of known city names (from a predefined list) with a standardized version.\n",
    "\n",
    "4. Drop Outliers:\n",
    "\n",
    "   * Remove rows with invalid city entries (e.g., 'Uk', 'Usa', etc.) and those with an empty state.\n",
    "\n",
    "5. Special Handling for Washington, D.C.:\n",
    "\n",
    "   * Standardize variants of \"Washington, D.C.\" to \"Washington DC\" and update the state to \"District Of Columbia.\"\n",
    "\n",
    "6. Manual Corrections:\n",
    "\n",
    "   * Manually fix specific cases where cleaning did not remove unwanted suffixes or stray characters.\n",
    "  \n",
    "* Some examples of standardized city expression:\n",
    "  * New York:\n",
    "    * New York City And San Diego\n",
    "    * New York City Area\n",
    "    * New York City Office\n",
    "    * New York Ny\n",
    "    * New York City Metropolitan Area\n",
    "    * New-York + Los Angeles\n",
    "    * New York Metro Area (Short Hills)\n",
    "    * New York City -4 Yrs & Atlanta\n",
    "  * 300 Madison Avenue:\n",
    "    * 300 Madison Avenue New York\n",
    "    * 300 Madison Ave\n",
    "    * 300 Madison Ave.\n",
    "    * 300 Madison Ave\n",
    "  * Los Angeles:\n",
    "    * Los Angeles & San Jose\n",
    "    * Los Angeles Ca\n",
    "    * Los Angeles County\n",
    "    * Los Angeles Metropolitan Area\n",
    "    * Los Angeles/San Francisco   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Create \"office_key_location\" column (combination of AUDITOR_FKEY + office_location)\n",
    "def extract_city_location(x):\n",
    "    \"\"\"\n",
    "    Extracts the city part from a raw location string.\n",
    "    If a comma is present, extract the substring before the first comma,\n",
    "    trim whitespace, and convert to title case. Otherwise, just trim and title-case.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str) and ',' in x:\n",
    "        return x.split(',')[0].strip().title()\n",
    "    else:\n",
    "        return x.strip().title()\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes the text string by performing several steps:\n",
    "    1. Remove extra whitespace and unwanted punctuation:\n",
    "       - Trim whitespace.\n",
    "       - Remove content within parentheses.\n",
    "       - Replace punctuation with spaces.\n",
    "       - Remove extra spaces.\n",
    "       - Remove specific unwanted words (e.g., 'pwc', 'remote', 'township of', 'U S A', 'Greater').\n",
    "    2. Replace abbreviations with full names:\n",
    "       - 'ave' becomes 'avenue'\n",
    "       - 'ny' or 'nyc' becomes 'New York'\n",
    "       - 'ca' becomes 'California'\n",
    "       - 'Nj' becomes 'New Jersey'\n",
    "       - 'Wv' becomes 'West Virginia'\n",
    "    3. Correct typos and handle specific cases:\n",
    "       - Replace common misspellings and variants (e.g., 'Wahsington' -> 'Washington',\n",
    "         'Appletonoshkoshneenah' -> 'Appleton', 'Chcago' -> 'Chicago', etc.).\n",
    "       - Handle cases like \"Baltimore And Los Angeles\" by converting them to \"Los Angeles\".\n",
    "    4. Standardize names to match Audit Analytics data:\n",
    "       - Convert variants like 'Champaign County' to 'Champaign', 'Metro Park' to 'Metropark',\n",
    "         'Monmouth' to 'Monmouth Beach', etc.\n",
    "    5. Remove extra suffixes:\n",
    "       - Remove unwanted suffixes (listed in remove_suffixes) from the end of the string,\n",
    "         provided the suffix is not the entire string.\n",
    "    6. Final formatting:\n",
    "       - Remove any remaining extra spaces, trim the string, and convert to title case.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return text\n",
    "    \n",
    "    # Step 1: Remove punctuation and unwanted words\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # Remove parentheses and their content\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())  # Remove extra spaces\n",
    "    text = re.sub(r'\\bpwc\\b', '', text.strip(), flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bremote\\b', '', text.strip(), flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\btownship of\\b\", '', text.strip(), flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bU S A\\b\", '', text.strip(), flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bGreater\\b\", '', text.strip(), flags=re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "\n",
    "    # Step 2: Replace abbreviations\n",
    "    text = re.sub(r'\\bave\\b', 'avenue', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bny\\b', 'New York', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bca\\b', 'California', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bNj\\b', 'New Jersey', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bnyc\\b', 'New York', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bWv\\b', 'West Virginia', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Correct typos and handle specific cases\n",
    "    text = re.sub(r'\\bWahsington\\b', 'Washington', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bAppletonoshkoshneenah\\b', 'Appleton', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bBaltimore And Los Angeles\\b', 'Los Angeles', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bBethseda\\b', 'Bethesda', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bBogot\\b', 'Bogota', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bChcago\\b', 'Chicago', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bClark Summit\\b', 'Clarks Summit', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bDalls\\b', 'Dallas', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bFlorhampark\\b', 'Florham Park', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bIndinapolis\\b', 'Indianapolis', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bLee S Summit\\b', 'Lees Summit', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bLos Angele\\b', 'Los Angeles', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMetropark\\b', 'Metro Park', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMilwakukee\\b', 'Milwaukee', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMilwuakee\\b', 'Milwaukee', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bPhiladephia\\b', 'Philadelphia', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bPhoenx Az\\b', 'Phoenix', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRanch Cucamonga\\b', 'Rancho Cucamonga', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRancho Cucamona\\b', 'Rancho Cucamonga', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRgion De Binghamton\\b', 'Binghamton', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRgion De La Baie De San Francisco\\b', 'San Francisco', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRgion De New York\\b', 'New York', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bRgion De Washington D C Metro\\b', 'Washington D C', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bPhila\\b', 'Philadelphia', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bTyson\\b', 'Tysons', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bAubun Hills\\b', 'Auburn Hills', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bTyson\\b', 'Tysons', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMaryland And Pennsylvania Offices\\b', 'Maryland', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bSan Jos\\b', 'San Jose', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bLouisvlle\\b', 'Louisville', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMiasmi\\b', 'Miami', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bNew Olreans\\b', 'New Orleans', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bPhladelphia\\b', 'Philadelphia', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bWilkes Bare\\b', 'Wilkes Barre', text, flags=re.IGNORECASE)\n",
    "   \n",
    "    # Step 4: Standardize to Audit Analytics city names\n",
    "    text = re.sub(r'\\bChampaign County\\b', 'Champaign', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMetro Park\\b', 'Metropark', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bMonmouth\\b', 'Monmouth Beach', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bOklahoma\\b$', 'Oklahoma City', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bSalt Lake County\\b$', 'Salt Lake CITY', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bShort Hill\\b', 'Short Hills', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bTyson s\\b', 'Tysons', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bWoodland\\b', 'Woodland Hills', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bKansas\\b$', 'Kansas City', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b3 Embarcadero Center San Francisco\\b', 'San Francisco', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b465 California St San Francisco\\b', 'San Francisco', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 5: Remove extra suffixes\n",
    "    remove_suffixes = [\n",
    "        \"new york\", \"junction\", \"maine\", \"md\", \"irvine\", \"normal\", \"indianapolis\",\n",
    "        \"fl\", \"d c\", \"denver\", \"metropolitan area\", \"area\", \"north carolina\", \"oh\", \"wv\",\n",
    "        \"ohio\", \"nj\", \"region\", \"sc\", \"indiana\", \"nv\", 'usa', \"ne\", \"arkansas\", \"ky\",\n",
    "        \"va\", \"new jersey\", \"florida\", \"metropolitan\", \"wi\", \"minnesota\", \"office newark\",\n",
    "        \"ct\", \"az\", \"pa\", \"mi\", \"ri\", \"mn\", \"wa\", \"ut\", \"Il\", \"Ii\", \"ll\", \"li\", \"ma\", \"virginia\"\n",
    "    ]\n",
    "\n",
    "    # Remove each suffix from the end if it's not the entire string\n",
    "    for suffix in remove_suffixes:\n",
    "        if text.lower() != suffix:\n",
    "            text = re.sub(fr'\\b{re.escape(suffix)}\\b$', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 6: Final formatting\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    text = text.title()  # Convert to title case\n",
    "    return text\n",
    "\n",
    "def standardize_location(df):\n",
    "    \"\"\"\n",
    "    Standardizes the office city names using a predefined list of city names.\n",
    "    For each city in the list, if the cleaned city name starts with that city,\n",
    "    replace the entire string with the standardized city name.\n",
    "    \"\"\"\n",
    "    cities = [\n",
    "        \"Atlanta\", \"Antwerp\", \"Appleton\", \"New York\", \"Newport Beach\", \"Baltimore\", \"Baton Rouge\",\n",
    "        \"Bogota\", \"Boise\", \"Boston\", \"Buffalo Niagara\", \"Cape Coral\", \"Century\", \"Chicago\",\n",
    "        \"Cincinnati\", \"College Station\", \"Columbus\", \"Conroe\", \"Dallas\", \"Denver\", \"Des Moines\",\n",
    "        \"Destin\", \"Detroit\", \"Downtown\", \"Dublin\", \"Eau Claire\", \"El Paso\", \"Fargo\", \"Ft Lauderdale\",\n",
    "        \"Gainesville\", \"Galway\", \"Ghent\", \"Gothenburg\", \"Grand Rapids\", \"Greater Boston\",\n",
    "        \"Greater Chicago\", \"Greater Los Angeles\", \"Greater Minneapolis\", \"Greater New York\",\n",
    "        \"Greater Philadelphia\", \"Greater San Diego\", \"Greater Washington\", \"Hartford\", \"Haverford\",\n",
    "        \"Irvine\", \"Janesville\", \"Kansas City\", \"Knoxville\", \"La Jolla\", \"Las Vegas\", \"Leeds\",\n",
    "        \"Limerick\", \"Little Rock\", \"London\", \"Los Angeles\", \"Louisville\", \"Memphis\",\n",
    "        \"Miami Fort Lauderdale\", \"Mobile\", \"Nashville\", \"National\", \"Norfolk\", \"Nueva York\",\n",
    "        \"Oakland\", \"Omaha\", \"Orlando\", \"Pensacola\", \"Philadelphia\", \"Phoenix\", \"Radnor\",\n",
    "        \"Raleigh Durham Chapel Hill\", \"Richmond\", \"Sacramento\", \"San Diego\", \"San Francisco Bay\",\n",
    "        \"San Ramon\", \"Shreveport\", \"South Bend\", \"Springfield\", \"St Louis\", \"Stamford\",\n",
    "        \"State College\", \"Syracuse\", \"Tallahassee\", \"Tampa\", \"Texas\", \"Troy\", \"Tulsa\", \"Tysons\",\n",
    "        \"Urbana\", \"Wausau\", \"Avon\", \"Bowling Green\", \"Byron\", \"California\", \"Charlotte\",\n",
    "        \"Clarksburg\", \"Dayton\", \"Englewood\", \"Fort Lauderdale\", \"Greater Detroit\",\n",
    "        \"Greater Milwaukee\", \"Greater New Orleans\", \"Greater Pittsburgh\", \"Greater St Louis\",\n",
    "        \"Greensboro Winston Salem High Point\", \"Greenville\", \"Houston\", \"Iowa City Cedar Rapids\",\n",
    "        \"Killeen Temple\", \"Lexington\", \"Lincoln\", \"Macomb\", \"Milwaukee\", \"New London\", \"San Jose\",\n",
    "        \"Skokie\", \"Buffalo\", \"Cleveland\", \"Fort Worth\", \"Greensboro\", \"Hickory\", \"Huntington\",\n",
    "        \"Lancaster\", \"Mclean\", \"Miami\", \"Midland\", \"Minneapolis\", \"New Haven\", \"Orange County\",\n",
    "        \"Peoria\", \"Portland\", \"Providence\", \"Raleigh\", \"Rochester\", \"Santa Clara\", \"Spartanburg\",\n",
    "        \"Spokane\", \"New Orleans\", \"Oklahoma City\", \"Salt Lake City\"\n",
    "    ]\n",
    "\n",
    "    # Replace any string starting with a city name with the standardized name\n",
    "    for city in cities:\n",
    "        df['office_city_cleaned'] = df['office_city_cleaned'].str.replace(\n",
    "            fr'^{re.escape(city)}\\b.*', city, case=False, regex=True\n",
    "        )\n",
    "\n",
    "    df['office_city_cleaned'] = df['office_city_cleaned'].str.title().str.strip()\n",
    "    return df\n",
    "\n",
    "def drop_outlier(df):\n",
    "    \"\"\"\n",
    "    Removes rows with invalid city names or empty states.\n",
    "    Specifically, drop rows where office_city_cleaned is one of ['Uk', 'Usa', 'Us', '757']\n",
    "    or where the state is marked as 'empty'.\n",
    "    \"\"\"\n",
    "    df = df[~df['office_city_cleaned'].isin(['Uk', 'Usa', 'Us', '757'])]\n",
    "    df = df[df['state'] != 'empty']\n",
    "    return df\n",
    "\n",
    "# Extract initial office city from raw location\n",
    "df_filtered['office_city_raw'] = df_filtered['location_raw'].apply(extract_city_location)\n",
    "\n",
    "# Clean the raw office city text\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_raw'].apply(clean_text)\n",
    "\n",
    "# Standardize office city names using a predefined list\n",
    "df_filtered = standardize_location(df_filtered)\n",
    "\n",
    "# Drop outlier rows with invalid city names or empty states\n",
    "df_filtered = drop_outlier(df_filtered)\n",
    "\n",
    "### Special handling for Washington, D.C.\n",
    "# Step 1: For rows where state is \"Washington, D.C.\" and the cleaned city contains \"Washington\",\n",
    "# set the office city to \"Washington DC\".\n",
    "mask1 = (\n",
    "    (df_filtered['state'] == \"Washington, D.C.\")\n",
    "    & (df_filtered['office_city_cleaned'].str.contains(r'\\bWashington\\b', case=False, na=False))\n",
    ")\n",
    "df_filtered.loc[mask1, 'office_city_cleaned'] = \"Washington DC\"\n",
    "\n",
    "# Step 2: Normalize variants of \"Washington DC\" using regex (ignoring case)\n",
    "mask2 = df_filtered['office_city_cleaned'].str.contains(r'(?i)\\bwashington\\s*d\\s*c\\b', na=False)\n",
    "df_filtered.loc[mask2, 'office_city_cleaned'] = 'Washington DC'\n",
    "\n",
    "# Step 3: For rows with \"Washington DC\", update the state to \"District Of Columbia\"\n",
    "mask3 = df_filtered['office_city_cleaned'].str.strip() == \"Washington DC\"\n",
    "df_filtered.loc[mask3, 'state'] = 'District Of Columbia'\n",
    "\n",
    "### Manual corrections for specific cases\n",
    "# Fix isolated \"New\" to \"New York\" (accounting for possible invisible characters)\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.strip()\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.replace(\n",
    "    r'^\\s*New\\s*$', 'New York', flags=re.IGNORECASE, regex=True\n",
    ")\n",
    "\n",
    "# Fix \"Peoria Metropolitan Area\" to \"Peoria\"\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.strip()\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.replace(\n",
    "    r'^\\s*Peoria Metropolitan Area\\s*$', 'Peoria', flags=re.IGNORECASE, regex=True\n",
    ")\n",
    "\n",
    "# Fix \"Greensboro Winston Salem\" variants to \"Greensboro\"\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.strip()\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.replace(\n",
    "    r'^\\s*Greensboro Winston Salem\\s*$', 'Greensboro', flags=re.IGNORECASE, regex=True\n",
    ")\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.replace(\n",
    "    r'^\\s*Greensboro Winston Salem High Point\\s*$', 'Greensboro', flags=re.IGNORECASE, regex=True\n",
    ")\n",
    "\n",
    "# Fix \"Hickory Lenoir\" to \"Hickory\"\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.strip()\n",
    "df_filtered['office_city_cleaned'] = df_filtered['office_city_cleaned'].str.replace(\n",
    "    r'^\\s*Hickory Lenoir\\s*$', 'Hickory', flags=re.IGNORECASE, regex=True\n",
    ")\n",
    "\n",
    "print(f\"There are {df_filtered['office_city_cleaned'].nunique()} unique cities after cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Create office_id and mas_id\n",
    "* Standardize the `location_raw` (when manually check, find out that same location have different experssions)\n",
    "  * If the `location_raw` string has \",\", only extract the string before first \",\"\n",
    "    * Those without \",\", get the entire string\n",
    "  * Then merge extracted string from `location_raw` with `state` to create `office_location`\n",
    "* Next, merge `AUDITOR_FKEY` with `office_location` to create office-level identification `office_key_location`\n",
    "* Use `office_key_location` to create `office_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Generate office_id\n",
    "# Ensure the \"AUDITOR_FKEY\" and \"AUDITOR_NAME\" columns are strings\n",
    "df_filtered[\"AUDITOR_FKEY\"] = df_filtered[\"AUDITOR_FKEY\"].astype(str)\n",
    "df_filtered[\"AUDITOR_NAME\"] = df_filtered[\"AUDITOR_NAME\"].astype(str)\n",
    "\n",
    "\n",
    "df_filtered['office_location'] = df_filtered['office_city_cleaned'].str.strip() + ',' + ' ' + df_filtered['state'].str.strip()\n",
    "df_filtered['office_key_location'] = df_filtered['AUDITOR_FKEY'] + ',' + ' ' + df_filtered['office_location'] \n",
    "\n",
    "# Create \"office_fullname\" column (AUDITOR_NAME + loffice_location)\n",
    "df_filtered[\"office_fullname\"] = df_filtered[\"AUDITOR_NAME\"].str.title() + \" \" + df_filtered['office_location'].str.title()\n",
    "# Create unique IDs \"office_id\" for each unique \"'office_key_location'\"\n",
    "df_filtered[\"office_id\"] = df_filtered['office_key_location'].factorize()[0] + 1  # Starts from 1\n",
    "\n",
    "\n",
    "# 2. Order and save the individual-level panel data\n",
    "# Define the columns to move to the beginning\n",
    "cols_to_move = [\"user_id\", \n",
    "                \"year\", \n",
    "                \"role_k1500\", \n",
    "                \"office_id\", \n",
    "                \"office_fullname\"] \n",
    "# Get the remaining columns (excluding those moved)\n",
    "remaining_cols = [col for col in df_filtered.columns if col not in cols_to_move]\n",
    "\n",
    "# Reorder DataFrame\n",
    "df_filtered = df_filtered[cols_to_move + remaining_cols]\n",
    "\n",
    "# 3. Create metro_ID for each MSA\n",
    "df_filtered['metro_area'] = df_filtered['metro_area'].str.strip().str.title() # Striping space and Capitalizing the first letter of each word\n",
    "df_filtered['metro_id'] = df_filtered['metro_area'].factorize()[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Keep only individuals who have ever worked as an auditor\n",
    "  * Create `ever_auditor` to flag individuals who have ever worked as an auditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️. Sort panel data by individual and year to ensure chronological order\n",
    "df_filtered = df_filtered.sort_values(by=['user_id', 'year'])\n",
    "\n",
    "# 2️. Define \"Ever Auditor\" - If a user was an auditor at any time, they will be defined as an auditor\n",
    "df_filtered[\"ever_auditor\"] = df_filtered.groupby(\"user_id\")[\"role_k1500\"].transform(lambda x: (x == \"auditor\").any())\n",
    "\n",
    "# 3. Only preserve individuals that are \"Ever Auditor\"\n",
    "df_auditor = df_filtered[df_filtered[\"ever_auditor\"] == True].copy() # Create a new Dataframe using .copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a summary across the entire panel\n",
    "total_indivi = df_filtered[\"user_id\"].nunique()  # Total number of individuals\n",
    "total_auditors = df_filtered.loc[df_filtered[\"ever_auditor\"], \"user_id\"].nunique()  # Total number of auditors\n",
    "\n",
    "# Print summary\n",
    "print(f\"There are {total_indivi} individuals in the dataframe.\")\n",
    "print(f\"There are {total_auditors} auditors in the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Check how many auditors work at multiple locations based on characters\n",
    "\n",
    "* Defined as an auditor whose 'office_city_raw' column contains \"&\", \"/\", \"and\", or \"+\"\n",
    "* Create an indicator `multi_place` to flag an auditor who is likely to work at multiple locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_city_location(x):\n",
    "    \"\"\"\n",
    "    Extracts the city from the location string.\n",
    "    If the string contains a comma, it returns the substring before the first comma.\n",
    "    Otherwise, it returns the entire string after stripping whitespace and converting it to title case.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str) and ',' in x:\n",
    "        return x.split(',')[0].strip().title()\n",
    "    else:\n",
    "        return x.strip().title()\n",
    "\n",
    "# Extract the original city information from the raw location data\n",
    "df_auditor['office_city_raw'] = df_auditor['location_raw'].apply(extract_city_location)\n",
    "\n",
    "# Calculate the total number of unique US auditors in Revelio\n",
    "total_auditor_number = df_auditor['user_id'].nunique()\n",
    "\n",
    "# Define the list of patterns that indicate multiple office locations\n",
    "patterns = ['&', 'and', '+', '/']\n",
    "\n",
    "# Escape any special regex characters in the patterns\n",
    "escaped_patterns = [re.escape(pattern) for pattern in patterns]\n",
    "\n",
    "# Create a mask to check if the 'office_city_raw' column (in lowercase) contains any of the specified patterns\n",
    "mask = df_auditor['office_city_raw'].str.lower().str.contains('|'.join(escaped_patterns), na=False)\n",
    "\n",
    "# Filter the DataFrame to include only rows that match the condition (auditors with multi-location indicators)\n",
    "filtered_df = df_auditor[mask]\n",
    "total_auditor_number_with_multi_office = filtered_df['user_id'].nunique()\n",
    "\n",
    "# Print the summary statistics\n",
    "print(f\"Total number of US auditors in Revelio is {total_auditor_number}\\n\"\n",
    "      f\"Total number of US auditors with multi-location is {total_auditor_number_with_multi_office}\\n\"\n",
    "      f\"The proportion is {total_auditor_number_with_multi_office / total_auditor_number}\")\n",
    "\n",
    "# Add a new indicator column \"multi_place\" to df_auditor.\n",
    "# For each auditor (user_id), if any of their records in 'office_city_raw' contains one of the specified patterns,\n",
    "# then mark all records for that auditor with a 1; otherwise, mark them with a 0.\n",
    "df_auditor['multi_place'] = df_auditor.groupby('user_id')['office_city_raw'].transform(\n",
    "    lambda x: int(x.str.lower().str.contains('|'.join(escaped_patterns), na=False).any())\n",
    ")\n",
    "\n",
    "# Check the distribution of the new 'multi_place' column\n",
    "print(df_auditor[['user_id', 'office_city_raw', 'multi_place']].drop_duplicates('user_id').head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4.1 Manually Check the office_city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the original city-state expression in Revelio data\n",
    "city_report = df_auditor.groupby(['office_city_raw', 'state'], as_index=False).size().sort_values(by='office_city_raw').reset_index(drop=True)\n",
    "\n",
    "print(f\"raw has {len(city_report)} rows \\n\")\n",
    "      \n",
    "#show(city_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export a list of cleaned city name for manually checking with city list in AA data\n",
    "city_list = df_auditor[['office_city_raw', 'state']].drop_duplicates(subset=['office_city_raw', 'state']).sort_values(by=['office_city_raw', 'state'])\n",
    "print(f\"There are {len(city_list)} cities in dataframe before cleaning\")\n",
    "\n",
    "city_list.to_csv(r\"E:\\USA auditor turnover data\\turnover data\\revelio_city_list_raw.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the cleaned city-state expression in Revelio\n",
    "city_report_clean = df_auditor.groupby(['office_city_cleaned', 'state'], as_index=False).size().sort_values(by='office_city_cleaned').reset_index(drop=True)\n",
    "\n",
    "print(f\"clean has {len(city_report_clean)} rows \\n\")\n",
    "\n",
    "#show(city_report_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export a list of cleaned city name for check with city list in AA data\n",
    "city_list_clean = df_auditor[['office_city_cleaned', 'state']].drop_duplicates().sort_values(by=['office_city_cleaned', 'state'])\n",
    "print(f\"There are {len(city_list_clean)} cities in dataframe after cleaning\")\n",
    "\n",
    "city_list_clean.to_excel(r\"E:\\USA auditor turnover data\\turnover data\\revelio_city_list_cleaned_final.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Drop rows with missing value in metro_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'metro_area' column is missing (NaN)\n",
    "len7 = len(df_auditor)\n",
    "df_auditor = df_auditor.dropna(subset=['metro_area'])\n",
    "len8 = len(df_auditor)\n",
    "\n",
    "print(f\"{len7 - len8} rows dropped because of missing metro_area\")\n",
    "print(f\"And {len8} rows left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Drop rows with missing value in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'state' column is missing (NaN)\n",
    "len9 = len(df_auditor)\n",
    "df_auditor = df_auditor.dropna(subset=['state'])\n",
    "len10 = len(df_auditor)\n",
    "\n",
    "print(f\"{len9 - len10} rows dropped because of missing state\")\n",
    "print(f\"And {len10} rows left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save cleaned ever auditor-panel csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"E:\\USA auditor turnover data\\turnover data\\CV_US_auditor.csv\"\n",
    "df_auditor.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_auditor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Calculate office-year panel variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Identify auditors that work in multi-office\n",
    "* Create an indicator that label auditors who indeed work at multi-office in the same year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in csv file\n",
    "input_path = r\"E:\\USA auditor turnover data\\turnover data\\CV_US_auditor.csv\"\n",
    "df_ever_auditor = pd.read_csv(input_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each auditor in the same year, calculate the number of unique office_ids\n",
    "df_ever_auditor['office_count_in_year'] = df_ever_auditor.groupby(['user_id', 'year'])['office_id'].transform('nunique')\n",
    "\n",
    "# Mark as 1 if the auditor has more than one office in the same year; otherwise, mark as 0\n",
    "df_ever_auditor['multi_office'] = (df_ever_auditor['office_count_in_year'] > 1).astype(int)\n",
    "\n",
    "# Calculate the number of auditors who worked in multiple offices in the same year\n",
    "auditor_multi = df_ever_auditor.groupby('user_id')['multi_office'].max()\n",
    "n_multi = (auditor_multi == 1).sum()\n",
    "n_total = df_ever_auditor['user_id'].nunique()\n",
    "\n",
    "print(f\"Number of auditors: {n_total}\")\n",
    "print(f\"Number of auditors working at multiple offices in the same year: {n_multi}\")\n",
    "print(f\"Proportion: {n_multi/n_total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two datasets: one including all auditors, and one excluding auditors who worked in multiple offices in a year\n",
    "save_path1 = r\"E:\\USA auditor turnover data\\turnover data\\revelio_auditor_us.csv\"\n",
    "save_path2 = r\"E:\\USA auditor turnover data\\turnover data\\revelio_auditor_us_filter.csv\"\n",
    "\n",
    "df_include = df_ever_auditor\n",
    "df_exclude = df_ever_auditor[(df_ever_auditor['multi_office'] != 1) & (df_ever_auditor['multi_place'] != 1)]\n",
    "\n",
    "df_include.to_csv(save_path1, index=False)\n",
    "df_exclude.to_csv(save_path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Calculate auditor flow\n",
    "* Flow variables：\n",
    "  * $\\text{Number of flow in auditor}_{t} = \\text{Number of auditors(i.e., user\\_id) that appear at office in year}_{t}\\ \\text{but do not appear in year}_{t-1}$\n",
    "  * $\\text{Number of flow out auditor}_{t} = \\text{Number of auditors(i.e., user\\_id) that appear at office in year}_{t-1}\\ \\text{but do not appear in year}_{t}$\n",
    "  * For start year $t$ (i.e., when there is no auditors' record from the previous year):\n",
    "    * $\\text{Flow in rate}_{t} = \\cfrac{\\text{Number of flow in auditors}_{t}}{\\text{Total number of auditor}_{t}} = 1$\n",
    "    * $\\text{Flow out rate = 0}$\n",
    "  * For none start year $t$\n",
    "    * $ \\text{Flow in(out) rate}_{t} = \\cfrac{\\text{Number of flow in(out) auditors}_{t}}{\\text{Total number of auditors}_{t-1}}$\n",
    "\n",
    "* Create an indicator `gap_indicator_revelio`/`gap_indicator_revelio_row`：\n",
    "  * `gap_indicator_revelio`: For an office that has ever experienced a discontinuous auditor working history (i.e., total auditors == 0 in any year), mark it with 1; otherwise, 0.\n",
    "  * `gap_indicator_revelio_row`: For an office that has ever experienced a discontinuous auditor working history (i.e., total auditors == 0 in any year), mark its specific gap‐filled rows with 1; otherwise, 0.\n",
    "* Note:\n",
    "  * The calculation is performed under two conditions:\n",
    "    1. Including auditors working in multi-place or multi-office.\n",
    "    2. Excluding auditors working in multi-place or multi-office.\n",
    "  * If an auditor have working history gap, but after the gap, they still work at the same office:\n",
    "    * Then treat them as there is no gap, fill the missing row with the same value before the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.1 Handle the auditor have working history gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the auditor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "input_path1 = r\"E:\\USA auditor turnover data\\turnover data\\revelio_auditor_us.csv\"\n",
    "input_path2 = r\"E:\\USA auditor turnover data\\turnover data\\revelio_auditor_us_filter.csv\"\n",
    "\n",
    "df_ever_auditor = pd.read_csv(input_path1, engine='pyarrow')\n",
    "df_ever_auditor_filtered = pd.read_csv(input_path2, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check if there are auditors that have working history gap, even they no long work as an auditor\n",
    "  * summary_df: Total number of auditors have gap in each year\n",
    "  * detailed_df: Individuals who have gap and they detail information: user_id, gap range etc.\n",
    "  * auditor_only_df: Individuals who have gap and continue working in the same office after the gap, and they detail information: user_id, gap range etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. Identify Gaps in Auditor Work History\n",
    "def compute_auditor_working_gap(df):\n",
    "    \"\"\"\n",
    "    Identifies auditors who had a discontinuous working history but returned to the same office after the gap.\n",
    "    Also computes the time series of the number of auditors with gaps per year.\n",
    "\n",
    "    Parameters:\n",
    "      df: DataFrame containing auditor data with 'office_id', 'year', 'user_id', 'role_k1500'.\n",
    "\n",
    "    Returns:\n",
    "      - summary_df: Time series data showing the count of auditors with gaps per year.\n",
    "      - detailed_df: Detailed records of auditors with gaps.\n",
    "      - auditor_only_df: Filtered version containing only auditors who remained in the same office.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Iterate over each auditor\n",
    "    for auditor, group in df.groupby(\"user_id\"):\n",
    "        group = group.sort_values(\"year\")\n",
    "        years = group[\"year\"].tolist()\n",
    "        offices = group.set_index(\"year\")[\"office_id\"].to_dict()\n",
    "        roles = group.set_index(\"year\")[\"role_k1500\"].to_dict()\n",
    "\n",
    "        # Detect gaps in employment history\n",
    "        gaps = [(years[i], years[i+1]) for i in range(len(years) - 1) if years[i+1] - years[i] > 1]\n",
    "\n",
    "        for gap_start, gap_end in gaps:\n",
    "            if gap_start in offices and gap_end in offices:\n",
    "                if offices[gap_start] == offices[gap_end]:  # Same office before and after the gap\n",
    "                    results.append({\n",
    "                        \"year\": gap_end,\n",
    "                        \"user_id\": auditor,\n",
    "                        \"office_id\": offices[gap_end],\n",
    "                        \"gap_start_year\": gap_start,\n",
    "                        \"gap_end_year\": gap_end,\n",
    "                        \"was_auditor_before\": roles.get(gap_start) == \"auditor\",\n",
    "                        \"was_auditor_after\": roles.get(gap_end) == \"auditor\"\n",
    "                    })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    detailed_df = pd.DataFrame(results)\n",
    "\n",
    "    if detailed_df.empty:\n",
    "        print(\"No auditors found with discontinuous work history staying in the same office.\")\n",
    "        return pd.DataFrame(columns=[\"year\", \"num_auditors_with_gaps\", \"num_auditors_continuous\"]), \\\n",
    "               pd.DataFrame(columns=[\"year\", \"user_id\", \"office_id\", \"gap_start_year\", \"gap_end_year\"]), \\\n",
    "               pd.DataFrame(columns=[\"year\", \"user_id\", \"office_id\", \"gap_start_year\", \"gap_end_year\"])\n",
    "\n",
    "    # Filter auditors who were auditors before and after the gap\n",
    "    auditor_only_df = detailed_df[(detailed_df[\"was_auditor_before\"]) & (detailed_df[\"was_auditor_after\"])].copy()\n",
    "    auditor_only_df = auditor_only_df.drop(columns=[\"was_auditor_before\", \"was_auditor_after\"])\n",
    "\n",
    "    # Compute the number of auditors with gaps per year\n",
    "    summary_df_all = detailed_df.groupby(\"year\")[\"user_id\"].nunique().reset_index()\n",
    "    summary_df_all.rename(columns={\"user_id\": \"num_auditors_with_gaps\"}, inplace=True)\n",
    "\n",
    "    # Compute the number of auditors whose role remained unchanged before and after the gap\n",
    "    summary_df_continuous = auditor_only_df.groupby(\"year\")[\"user_id\"].nunique().reset_index()\n",
    "    summary_df_continuous.rename(columns={\"user_id\": \"num_auditors_continuous\"}, inplace=True)\n",
    "\n",
    "    # Merge the two summary statistics\n",
    "    summary_df = summary_df_all.merge(summary_df_continuous, on=\"year\", how=\"left\").fillna(0)\n",
    "    summary_df[\"num_auditors_continuous\"] = summary_df[\"num_auditors_continuous\"].astype(int)\n",
    "\n",
    "    return summary_df, detailed_df, auditor_only_df\n",
    "\n",
    "\n",
    "# Execute Auditor Gap Analysis\n",
    "summary_df_ever, detailed_df_ever, auditor_only_df_ever = compute_auditor_working_gap(df_ever_auditor)\n",
    "summary_df_filtered, detailed_df_filtered, auditor_only_df_filtered = compute_auditor_working_gap(df_ever_auditor_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary results\n",
    "print(\"Summary of auditors with gaps in df_ever_auditor:\")\n",
    "print(summary_df_ever.head())\n",
    "\n",
    "print(\"Summary of auditors with gaps in df_ever_auditor_filtered:\")\n",
    "print(summary_df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auditor_only_df_ever.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill the gaps in auditor work history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_auditor_gaps(df, detailed_df):\n",
    "    \"\"\"\n",
    "    Fills gaps for auditors who had a discontinuous working history but remained in the same office.\n",
    "    Ensures that the last available record is only copied once and avoids duplication.\n",
    "\n",
    "    Parameters:\n",
    "      df: Original DataFrame of auditor records.\n",
    "      detailed_df: DataFrame of auditors with identified gaps.\n",
    "\n",
    "    Returns:\n",
    "      df_filled: Updated DataFrame with missing years filled.\n",
    "    \"\"\"\n",
    "    df_filled = df.copy()\n",
    "    rows_to_add = []\n",
    "\n",
    "    for _, row in detailed_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        office_id = row[\"office_id\"]\n",
    "        gap_start = row[\"gap_start_year\"]\n",
    "        gap_end = row[\"gap_end_year\"]\n",
    "\n",
    "        # Get the last available record before the gap\n",
    "        previous_records = df[\n",
    "            (df[\"user_id\"] == user_id) &\n",
    "            (df[\"year\"] < gap_start) &  # Only before the gap\n",
    "            (df[\"office_id\"] == office_id)\n",
    "        ]\n",
    "\n",
    "        if previous_records.empty:\n",
    "            continue  # Skip if no record exists before the gap\n",
    "\n",
    "        # Get the most recent record before the gap\n",
    "        previous_record = previous_records.sort_values(\"year\", ascending=False).iloc[0].to_dict()\n",
    "\n",
    "        # Fill missing years (without copying the last available year again)\n",
    "        for fill_year in range(gap_start+1, gap_end): # +1 Ensure we do not duplicate\n",
    "            if fill_year == previous_record[\"year\"]:  # Ensure we do not duplicate\n",
    "                continue\n",
    "            new_record = previous_record.copy()\n",
    "            new_record[\"year\"] = fill_year  # Update to the missing year\n",
    "            rows_to_add.append(new_record)\n",
    "\n",
    "    # Append newly created rows and return updated DataFrame\n",
    "    if rows_to_add:\n",
    "        df_filled = pd.concat([df_filled, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "\n",
    "    return df_filled\n",
    "\n",
    "\n",
    "# Apply gap-filling function\n",
    "df_ever_auditor_filled = fill_auditor_gaps(df_ever_auditor, auditor_only_df_ever)\n",
    "df_ever_auditor_filtered_filled = fill_auditor_gaps(df_ever_auditor_filtered, auditor_only_df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the result of filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_ever_auditor_filled[df_ever_auditor_filled['user_id'] == 866086458]\n",
    "#show(df_check)\n",
    "#show(df_ever_auditor_filled.head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.2 Calculate the auditor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_office_year_flow(df_ever_auditor, df_filtered, office_metro=None):\n",
    "    \"\"\"\n",
    "    Computes turnover flow indicators at the office-year level based on auditor data.\n",
    "    Additionally, computes rates using both total auditors and total employees as denominators.\n",
    "\n",
    "    Parameters:\n",
    "      df_ever_auditor: DataFrame containing at least:\n",
    "          - office_id\n",
    "          - year\n",
    "          - user_id\n",
    "          - role_k1500 (to indicate if the record is for an auditor)\n",
    "      df_filtered: DataFrame containing the full panel of employees (not just auditors), \n",
    "                   used to compute total employees in each office-year.\n",
    "      office_metro: Optional DataFrame containing office-level metro information.\n",
    "\n",
    "    Returns:\n",
    "      df_office_year: DataFrame at the office-year level with turnover flow indicators,\n",
    "                      gap indicators, and rates using both auditors and employees as denominators.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure year column is an integer\n",
    "    df = df_ever_auditor.copy()\n",
    "    df['year'] = df['year'].astype(int)\n",
    "\n",
    "    # Step 2: Flag records that are auditors\n",
    "    df['is_auditor'] = df['role_k1500'] == 'auditor'\n",
    "\n",
    "    # Step 3: Group by (office_id, year, user_id) and take the max of is_auditor\n",
    "    df_grouped = (\n",
    "        df.groupby([\"office_id\", \"year\", \"user_id\"])['is_auditor']\n",
    "          .max()\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only rows where is_auditor is True\n",
    "    df_active = df_grouped[df_grouped['is_auditor'] == True]\n",
    "    auditor_sets = (\n",
    "        df_active.groupby([\"office_id\", \"year\"])['user_id']\n",
    "          .agg(lambda x: set(x))\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Step 4: Create a complete office-year list\n",
    "    complete_office_years_list = []\n",
    "    for office in df['office_id'].unique():\n",
    "        office_years = df.loc[df['office_id'] == office, 'year']\n",
    "        min_year, max_year = office_years.min(), office_years.max()\n",
    "        complete_years = pd.DataFrame({\n",
    "            'office_id': office,\n",
    "            'year': range(min_year, max_year + 1)\n",
    "        })\n",
    "        complete_office_years_list.append(complete_years)\n",
    "\n",
    "    full_df = pd.concat(complete_office_years_list, ignore_index=True)\n",
    "\n",
    "    # Left merge with auditor sets\n",
    "    office_year_full = full_df.merge(auditor_sets, on=[\"office_id\", \"year\"], how=\"left\")\n",
    "    office_year_full['user_id'] = office_year_full['user_id'].apply(lambda x: x if isinstance(x, set) else set())\n",
    "\n",
    "    # Identify gap-filled rows\n",
    "    original_pairs = df_active[['office_id', 'year']].drop_duplicates().copy()\n",
    "    original_pairs['original'] = 1\n",
    "    office_year_full = office_year_full.merge(original_pairs, on=['office_id', 'year'], how='left')\n",
    "    office_year_full['gap_indicator_revelio_row'] = np.where(office_year_full['original'].isna(), 1, 0)\n",
    "    office_year_full = office_year_full.drop(columns=['original'])\n",
    "\n",
    "    office_year_sets = office_year_full\n",
    "\n",
    "    # Step 5: Compute turnover indicators\n",
    "    result_list = []\n",
    "    for office, group in office_year_sets.groupby(\"office_id\"):\n",
    "        group = group.sort_values(\"year\")\n",
    "        previous_set = None\n",
    "        previous_total = None\n",
    "        for _, row in group.iterrows():\n",
    "            current_year = row[\"year\"]\n",
    "            current_set = row[\"user_id\"]\n",
    "            current_total = len(current_set)\n",
    "\n",
    "            if previous_set is None or previous_total == 0:\n",
    "                flow_in, flow_out = current_total, 0\n",
    "                base = current_total\n",
    "            else:\n",
    "                flow_in = len(current_set - previous_set)\n",
    "                flow_out = len(previous_set - current_set)\n",
    "                base = previous_total\n",
    "\n",
    "            net_flow = flow_in - flow_out\n",
    "\n",
    "            # Compute rates using auditors as denominator\n",
    "            flow_in_rate = flow_in / base if base > 0 else 0\n",
    "            flow_out_rate = flow_out / base if base > 0 else 0\n",
    "            net_flow_rate = net_flow / base if base > 0 else 0\n",
    "\n",
    "            result_list.append({\n",
    "                \"office_id\": office,\n",
    "                \"year\": current_year,\n",
    "                \"flow_in\": flow_in,\n",
    "                \"flow_out\": flow_out,\n",
    "                \"net_flow\": net_flow,\n",
    "                \"total_auditors\": current_total,\n",
    "                \"flow_in_rate\": flow_in_rate,\n",
    "                \"flow_out_rate\": flow_out_rate,\n",
    "                \"net_flow_rate\": net_flow_rate,\n",
    "                \"gap_indicator_revelio_row\": row[\"gap_indicator_revelio_row\"]\n",
    "            })\n",
    "\n",
    "            previous_set = current_set if current_total > 0 else set()\n",
    "            previous_total = current_total\n",
    "\n",
    "    df_office_year = pd.DataFrame(result_list)\n",
    "\n",
    "    # Step 6: Compute total employees per office-year\n",
    "    df_total_employees = (\n",
    "        df_filtered.groupby([\"office_id\", \"year\"])['user_id']\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={'user_id': 'total_employees'})\n",
    "    )\n",
    "\n",
    "    # Merge total employees into df_office_year\n",
    "    df_office_year = df_office_year.merge(df_total_employees, on=[\"office_id\", \"year\"], how=\"left\")\n",
    "\n",
    "    # Step 7: Compute rates using total employees as denominator\n",
    "    df_office_year[\"flow_in_rate_employee\"] = df_office_year[\"flow_in\"] / df_office_year[\"total_employees\"]\n",
    "    df_office_year[\"flow_out_rate_employee\"] = df_office_year[\"flow_out\"] / df_office_year[\"total_employees\"]\n",
    "    df_office_year[\"net_flow_rate_employee\"] = df_office_year[\"net_flow\"] / df_office_year[\"total_employees\"]\n",
    "\n",
    "    # Handle NaN cases where total_employees might be 0\n",
    "    df_office_year.fillna({\"total_employees\": 0, \"flow_in_rate_employee\": 0, \"flow_out_rate_employee\": 0, \"net_flow_rate_employee\": 0}, inplace=True)\n",
    "\n",
    "    # Step 8: Flag offices that have any year with zero auditors\n",
    "    df_office_year['gap_indicator_revelio'] = df_office_year.groupby('office_id')['total_auditors'] \\\n",
    "        .transform(lambda x: int((x == 0).any()))\n",
    "\n",
    "    # Step 9: Merge metro information if provided\n",
    "    if office_metro is not None:\n",
    "        df_office_year = df_office_year.merge(office_metro, on='office_id', how='left', indicator=True)\n",
    "        df_office_year = df_office_year.drop(columns=['_merge'])\n",
    "\n",
    "    return df_office_year\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    office_metro = df_ever_auditor[['office_id', 'metro_area', 'metro_id', 'office_fullname', \n",
    "                                    'office_key_location', 'state', 'office_city_cleaned']].drop_duplicates(subset=['office_id'])\n",
    "    df_office_year = compute_office_year_flow(df_ever_auditor_filled, df_filtered, office_metro)\n",
    "    df_office_year_filtered = compute_office_year_flow(df_ever_auditor_filtered_filled, df_filtered, office_metro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(df_office_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\USA auditor turnover data\\checking_the_gap_revelio.csv\"\n",
    "\n",
    "df_office_year.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting columns to verify\n",
    "print(df_office_year.columns, len(df_office_year), df_office_year.head())\n",
    "print(df_office_year_filtered.columns, len(df_office_year_filtered), df_office_year_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Calculate high salary variables\n",
    "* Create a dummy variables to identify if a office provide auditor with average salary that above the median of MSA-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_office_year_salary_indicator(df_ever_auditor):\n",
    "    \"\"\"\n",
    "    Computes the average salary of auditors by office and year, extracts the metro_area and metro_id for each office,\n",
    "    calculates the median of the office average salaries within each metro_id and year, and creates an indicator\n",
    "    'above_median_salary' (1 if the office's average salary is above the metro median salary, otherwise 0).\n",
    "\n",
    "    If an office in a given year has no auditor records, it will still appear in the output with office_avg_salary = 0.\n",
    "\n",
    "    Parameters:\n",
    "        df_ever_auditor (DataFrame): Input DataFrame containing auditor data with at least the following columns:\n",
    "            - role_k1500 (e.g., 'auditor')\n",
    "            - salary\n",
    "            - office_id\n",
    "            - year\n",
    "            - metro_area\n",
    "            - metro_id\n",
    "            \n",
    "    Returns:\n",
    "        office_year_avg (DataFrame): Office-year level DataFrame with the following columns:\n",
    "            - office_id\n",
    "            - year\n",
    "            - office_avg_salary: Average salary of auditors in the office for that year (0 if no record).\n",
    "            - metro_area: The metro area of the office.\n",
    "            - metro_id: The metro ID of the office.\n",
    "            - metro_median_salary: The median of office average salaries within the metro for that year.\n",
    "            - above_median_salary: Indicator (1 if office_avg_salary > metro_median_salary, else 0).\n",
    "    \"\"\"\n",
    "    # 1. Filter for auditor records\n",
    "    df_auditor = df_ever_auditor[df_ever_auditor['role_k1500'] == 'auditor'].copy()\n",
    "    \n",
    "    # 2. Calculate the average salary for each office and year\n",
    "    office_year_avg = df_auditor.groupby([\"office_id\", \"year\"]).agg(\n",
    "        office_avg_salary=('salary', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 3. Extract office information (metro_area and metro_id) and all years (from df_ever_auditor)\n",
    "    office_info = df_ever_auditor[['office_id', 'metro_area', 'metro_id']].drop_duplicates()\n",
    "    years = pd.DataFrame({'year': sorted(df_ever_auditor['year'].unique())})\n",
    "    \n",
    "    # 4. Create all combinations of offices and years (cross join) to ensure every office appears in every year\n",
    "    office_info['key'] = 1\n",
    "    years['key'] = 1\n",
    "    offices_years = pd.merge(office_info, years, on='key').drop(columns='key')\n",
    "    \n",
    "    # 5. Merge the existing office-year average salaries into the full combination; fill missing values with 0\n",
    "    office_year_avg = pd.merge(offices_years, office_year_avg, on=['office_id', 'year'], how='left')\n",
    "    office_year_avg['office_avg_salary'] = office_year_avg['office_avg_salary'].fillna(0)\n",
    "    \n",
    "    # 6. Compute the median of office average salaries within each metro_id and year\n",
    "    metro_year_median = office_year_avg.groupby([\"metro_id\", \"year\"]).agg(\n",
    "        metro_median_salary=('office_avg_salary', 'median')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 7. Merge the metro median back into the office-year data\n",
    "    office_year_avg = pd.merge(office_year_avg, metro_year_median, on=['metro_id', 'year'], how='left')\n",
    "    \n",
    "    # 8. Create an indicator: mark as 1 if the office average salary is greater than the metro median, otherwise 0\n",
    "    office_year_avg['above_median_salary'] = (office_year_avg['office_avg_salary'] > office_year_avg['metro_median_salary']).astype(int)\n",
    "    \n",
    "    return office_year_avg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume df_ever_auditor and df_ever_auditor_filtered are already defined\n",
    "    df_office_year_salary = compute_office_year_salary_indicator(df_ever_auditor_filled)\n",
    "    df_office_year_salary_filtered = compute_office_year_salary_indicator(df_ever_auditor_filtered_filled)\n",
    "    \n",
    "    # Print results for inspection\n",
    "    print(df_office_year_salary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Calculate number of office in MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_office_number_in_MSA(dataframe):\n",
    "    msa_office_count = dataframe.groupby(['metro_id', 'year']).agg(\n",
    "        metro_area=('metro_area', 'first'),\n",
    "        office_count_in_MSA=('office_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    return msa_office_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    msa_office_count = calculate_office_number_in_MSA(df_ever_auditor_filled)\n",
    "    msa_office_count_filtered = calculate_office_number_in_MSA(df_ever_auditor_filtered_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge and save the office-level variables panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merge the turnover and high salary DataFrames\n",
    "\n",
    "def merge_variables_dataframe(df_flow, df_salary, df_number):\n",
    "    # Merge turnover (df_flow) with salary (df_salary) data on office_id and year using an inner join\n",
    "    df_merge = df_flow.merge(\n",
    "        df_salary, \n",
    "        left_on=[\"office_id\", \"year\"],\n",
    "        right_on=[\"office_id\", \"year\"],\n",
    "        how=\"inner\",\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # Print the merge report to check the distribution of the _merge column\n",
    "    merge_report = df_merge['_merge'].value_counts()\n",
    "    print(merge_report)\n",
    "\n",
    "    # Drop columns generated by the merge and rename columns as needed\n",
    "    df_merge = df_merge.drop(columns=['metro_area_x', 'metro_id_x', 'metro_area_y'])\n",
    "    df_merge = df_merge.rename(columns={'metro_id_y': 'metro_id'})\n",
    "    df_merge = df_merge.drop(columns=['_merge'])\n",
    "\n",
    "    # 2. Merge the resulting DataFrame with the number of offices DataFrame (df_number)\n",
    "    df_merge = df_merge.merge(\n",
    "        df_number, \n",
    "        left_on=[\"metro_id\", \"year\"],\n",
    "        right_on=[\"metro_id\", \"year\"],\n",
    "        how=\"inner\",\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # Print the merge report for the second merge\n",
    "    merge_report = df_merge['_merge'].value_counts()\n",
    "    print(merge_report)\n",
    "    df_merge = df_merge.drop(columns=['_merge'])\n",
    "\n",
    "    # 3. Order the variable columns\n",
    "    # Define the columns to move to the beginning\n",
    "    cols_to_move = [\n",
    "        \"year\", \n",
    "        \"office_id\",\n",
    "        \"office_fullname\",\n",
    "        \"office_key_location\",\n",
    "        \"metro_id\",\n",
    "        \"metro_area\",\n",
    "        \"total_auditors\"\n",
    "    ]\n",
    "    # Get the remaining columns that are not in cols_to_move\n",
    "    remaining_cols = [col for col in df_merge.columns if col not in cols_to_move]\n",
    "    # Reorder the DataFrame so that the specified columns appear first\n",
    "    df_merge = df_merge[cols_to_move + remaining_cols]\n",
    "    \n",
    "    return df_merge\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_merge = merge_variables_dataframe(df_office_year, df_office_year_salary, msa_office_count)\n",
    "    df_merge_filtered = merge_variables_dataframe(df_office_year_filtered, df_office_year_salary_filtered, msa_office_count_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When checking the data, I notice that using auditor_fkey + city + state can not identify a unique office.\n",
    "  * They have duplicates in the MSA\n",
    "  * Therefore, I just keep the first result of the duplictates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate (year, office_id) pairs\n",
    "duplicates = df_merge[df_merge.duplicated(subset=['office_id', 'year'], keep=False)]\n",
    "\n",
    "# Print the number of duplicates\n",
    "print(f\"Number of duplicate (year, office_id) pairs: {duplicates.shape[0]}\")\n",
    "\n",
    "# Show some examples\n",
    "print(duplicates[['office_id', 'year']].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates, keeping only the first occurrence\n",
    "len11 = len(df_merge)\n",
    "df_merge = df_merge.drop_duplicates(subset=['office_id', 'year'], keep='first')\n",
    "len12 = len(df_merge)\n",
    "\n",
    "len13 = len(df_merge_filtered)\n",
    "df_merge_filtered = df_merge_filtered.drop_duplicates(subset=['office_id', 'year'], keep='first')\n",
    "len14 = len(df_merge_filtered)\n",
    "\n",
    "# Check again to confirm duplicates are removed\n",
    "print(f\"After removal, duplicate count: {df_merge.duplicated(subset=['office_id', 'year']).sum()}, dupulicates dropped count: {len11 - len12}, there are {len12} rows left\")\n",
    "print(f\"After removal, duplicate count: {df_merge_filtered.duplicated(subset=['office_id', 'year']).sum()}, dupulicates dropped count: {len13 - len14}, there are {len14} rows left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the final panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save office-year auditor turnover and salary data\n",
    "import pandas as pd\n",
    "\n",
    "save_path1 = r\"E:\\USA auditor turnover data\\turnover data\\office-year_auditor_turnover_us.csv\"\n",
    "save_path2 = r\"E:\\USA auditor turnover data\\turnover data\\office-year_auditor_turnover_us_filtered.csv\"\n",
    "\n",
    "print(\"Begin to save the csv file\")\n",
    "df_merge.to_csv(save_path1, index=False)\n",
    "df_merge_filtered.to_csv(save_path2, index=False)\n",
    "print(\"Successfully save the csv file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
